import os
import time
import feedparser
import google.generativeai as genai
from newspaper import Article, Config
from supabase import create_client, Client

# ================= â˜ï¸ äº‘ç«¯ç‰ˆé…ç½® (æ— ä»£ç†) â˜ï¸ =================

# 1. ç›´æ¥ä»ç¯å¢ƒå˜é‡è¯»å– Keys (GitHub ä¼šè‡ªåŠ¨æ³¨å…¥)
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# 2. æ–°é—»æº
RSS_FEEDS = [
    "https://techcrunch.com/feed/",
    "https://www.coindesk.com/arc/outboundfeeds/rss/",
    "https://www.cnbc.com/id/100003114/device/rss/rss.html" # æ·»åŠ äº†CNBC
]

# ================= åˆå§‹åŒ– =================
if not GOOGLE_API_KEY or not SUPABASE_KEY:
    raise ValueError("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° API Keyï¼Œè¯·æ£€æŸ¥ GitHub Secrets é…ç½®ï¼")

genai.configure(api_key=GOOGLE_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
MODEL_NAME = 'gemini-2.0-flash'

# ================= æ ¸å¿ƒå‡½æ•° (ä¿æŒä¸å˜) =================

def check_if_exists(url):
    try:
        response = supabase.table("news").select("id").eq("url", url).execute()
        return len(response.data) > 0
    except Exception as e:
        print(f"âš ï¸ DB Check Error: {e}")
        return False

def get_article_content(url):
    config = Config()
    config.browser_user_agent = 'Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36' # ä¼ªè£…æˆæ‰‹æœº
    config.request_timeout = 10
    try:
        article = Article(url, config=config)
        article.download()
        article.parse()
        return article.title, article.text
    except Exception:
        return None, None

def ai_summarize(title, content):
    system_instruction = """
    ä½ æ˜¯ä¸€ä½é‡‘èç§‘æŠ€æƒ…æŠ¥å®˜ã€‚è¯·å°†æ–°é—»æ€»ç»“ä¸ºMarkdownæ ¼å¼ï¼š
    ### âš¡ ä¸€å¥è¯æ ¸å¿ƒ
    ### ğŸ“‰ å…³é”®æ•°æ®
    ### ğŸ‚ ç†Š/ç‰› è¯„çº§
    ### ğŸ·ï¸ æ ‡ç­¾
    """
    model = genai.GenerativeModel(model_name=MODEL_NAME, system_instruction=system_instruction)
    try:
        response = model.generate_content(f"æ ‡é¢˜ï¼š{title}\n\nå†…å®¹ï¼š{content[:8000]}")
        return response.text
    except Exception as e:
        print(f"âŒ AI Error: {e}")
        return None

def save_to_supabase(title, url, summary, source):
    data = {"title": title, "url": url, "content_summary": summary, "original_source": source}
    try:
        supabase.table("news").insert(data).execute()
        print(f"âœ… Saved: {title[:20]}...")
    except Exception as e:
        print(f"âŒ Save Error: {e}")

# ================= ä¸»å¾ªç¯ =================

def run_pipeline():
    print(f"ğŸš€ Starting Cloud Pipeline...")
    for feed_url in RSS_FEEDS:
        print(f"ğŸŒŠ Reading RSS: {feed_url}")
        try:
            feed = feedparser.parse(feed_url)
            # æ¯æ¬¡åªå–æœ€æ–°çš„ 5 æ¡ï¼Œé˜²æ­¢è¶…æ—¶
            for entry in feed.entries[:5]:
                url = entry.link
                if check_if_exists(url):
                    print("   â­ï¸ Skipped (Exists)")
                    continue
                
                print("   ğŸ“¥ Downloading...")
                title, content = get_article_content(url)
                if content:
                    print("   ğŸ§  Analyzing...")
                    summary = ai_summarize(title, content)
                    if summary:
                        source = "TechCrunch" if "techcrunch" in feed_url else "Other"
                        save_to_supabase(title, url, summary, source)
                        time.sleep(2) # ç¤¼è²Œçˆ¬è™«
        except Exception as e:
            print(f"âš ï¸ RSS Parse Error: {e}")

if __name__ == "__main__":
    run_pipeline()