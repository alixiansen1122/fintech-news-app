import os
import time
import json
import re
import feedparser
import google.generativeai as genai
from newspaper import Article, Config
from supabase import create_client, Client

# ================= é…ç½®åŒºåŸŸ =================
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

RSS_FEEDS = [
    "https://techcrunch.com/feed/",
    "https://www.coindesk.com/arc/outboundfeeds/rss/",
    "https://www.cnbc.com/id/100003114/device/rss/rss.html"
]

if not GOOGLE_API_KEY or not SUPABASE_KEY:
    raise ValueError("âŒ API Key ç¼ºå¤±")

genai.configure(api_key=GOOGLE_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
MODEL_NAME = 'gemini-2.0-flash'

# ================= è¾…åŠ©å‡½æ•° =================

def clean_json_text(text):
    """æ¸…ç† AI å¯èƒ½è¿”å›çš„ Markdown æ ¼å¼ç¬¦å·ï¼Œæå–çº¯ JSON"""
    # ç§»é™¤ ```json å’Œ ``` 
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    return text.strip()

def check_if_exists(url):
    try:
        response = supabase.table("news").select("id").eq("url", url).execute()
        return len(response.data) > 0
    except Exception:
        return False

def get_article_content(url):
    config = Config()
    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    config.request_timeout = 10
    try:
        article = Article(url, config=config)
        article.download()
        article.parse()
        return article.title, article.text
    except Exception:
        return None, None

def ai_summarize_structured(title, content):
    """
    è®© AI è¿”å›ä¸¥æ ¼çš„ JSON æ ¼å¼
    """
    # å¼ºåˆ¶ AI è¾“å‡º JSON çš„ Prompt
    system_instruction = """
    ä½ æ˜¯ä¸€ä½é‡‘èæ•°æ®åˆ†æå¼•æ“ã€‚ä¸è¦è¾“å‡ºä»»ä½• Markdown æ ¼å¼æˆ–åºŸè¯ã€‚
    è¯·é˜…è¯»æ–°é—»ï¼Œè¿”å›ä¸”ä»…è¿”å›ä¸€ä¸ªç¬¦åˆ Python è§£ææ ‡å‡†çš„ JSON å­—ç¬¦ä¸²ã€‚
    
    JSON ç»“æ„è¦æ±‚ï¼š
    {
        "summary": "30å­—ä»¥å†…çš„ä¸­æ–‡ä¸€å¥è¯æ ¸å¿ƒæ‘˜è¦",
        "key_stats": "å…³é”®æ•°æ®ï¼ˆå¦‚é‡‘é¢ã€ç™¾åˆ†æ¯”ï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™å¡«'æ— '",
        "sentiment_score": ä¸€ä¸ªæ•´æ•° (-10 ä»£è¡¨æåº¦åˆ©ç©º, 0 ä»£è¡¨ä¸­æ€§, 10 ä»£è¡¨æåº¦åˆ©å¥½),
        "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"]
    }
    """
    
    model = genai.GenerativeModel(model_name=MODEL_NAME, system_instruction=system_instruction)
    
    try:
        # æˆªå–å‰ 6000 å­—ç¬¦é˜²æ­¢ Token æº¢å‡º
        response = model.generate_content(f"æ–°é—»æ ‡é¢˜ï¼š{title}\n\nå†…å®¹ï¼š{content[:6000]}")
        raw_text = response.text
        
        # æ¸…ç†å¹¶è§£æ JSON
        json_str = clean_json_text(raw_text)
        data = json.loads(json_str)
        return data
        
    except Exception as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å› Noneï¼Œè·³è¿‡è¿™æ¡æ–°é—»
        return None

def save_to_supabase(title, url, ai_data, source):
    """
    ç°åœ¨ ai_data æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæˆ‘ä»¬æŠŠå®ƒæ‹†è§£å­˜å…¥ä¸åŒåˆ—
    """
    # ç»„åˆä¸€ä¸‹ summary å†…å®¹ï¼Œä¿ç•™ä¹‹å‰çš„æ ¼å¼ä¹ æƒ¯ï¼ŒæŠŠå…³é”®æ•°æ®æ‹¼åœ¨åé¢
    full_summary = f"{ai_data['summary']}\n\n**å…³é”®æ•°æ®:** {ai_data['key_stats']}"
    
    data = {
        "title": title,
        "url": url,
        "content_summary": full_summary, # ä¿æŒå…¼å®¹
        "original_source": source,
        "sentiment_score": ai_data['sentiment_score'], # æ–°å¢ï¼šåˆ†æ•°
        "tags": ai_data['tags'] # æ–°å¢ï¼šæ ‡ç­¾æ•°ç»„
    }
    
    try:
        supabase.table("news").insert(data).execute()
        print(f"âœ… å…¥åº“æˆåŠŸ: {title[:20]}... [åˆ†æ•°: {ai_data['sentiment_score']}]")
    except Exception as e:
        print(f"âŒ å…¥åº“å¤±è´¥: {e}")

# ================= ä¸»å¾ªç¯ =================

def run_pipeline():
    print("ğŸš€ å¯åŠ¨ç»“æ„åŒ–æ•°æ®æŠ“å–...")
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:5]: # é™åˆ¶æ¯æ¬¡æ¯ä¸ªæºæŠ“5æ¡
                url = entry.link
                
                if check_if_exists(url):
                    print("   â­ï¸ è·³è¿‡ (å·²å­˜åœ¨)")
                    continue
                
                print("   ğŸ“¥ ä¸‹è½½ä¸­...")
                title, content = get_article_content(url)
                
                if content:
                    print("   ğŸ§  AI åˆ†æä¸­ (JSONæ¨¡å¼)...")
                    # è°ƒç”¨æ–°çš„ç»“æ„åŒ–åˆ†æå‡½æ•°
                    ai_data = ai_summarize_structured(title, content)
                    
                    if ai_data:
                        source = "TechCrunch" if "techcrunch" in feed_url else "CoinDesk"
                        save_to_supabase(title, url, ai_data, source)
                        time.sleep(2)
                        
        except Exception as e:
            print(f"âš ï¸ RSS é”™è¯¯: {e}")

if __name__ == "__main__":
    run_pipeline()